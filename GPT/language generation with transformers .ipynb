{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae19290-a560-4441-bc48-a209c798de04",
   "metadata": {},
   "source": [
    "# Lab 4: Language Generation with Transformers\n",
    "\n",
    "When predicting the next token, a GPT model can give us a score for all possible next tokens. We can use those probabilities to generate new text, potentially by selecting the most likely next token or by sampling using the probabilities. Let's see how that works.\n",
    "\n",
    "Let's say that we want to generate more text after the sequence below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864e8c6b-369b-418b-a275-bff6ff382590",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The quick brown fox jumped over'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b5e07-6259-4a86-97fb-e193159a839e",
   "metadata": {},
   "source": [
    "We'll need to load the tokenizer and model for `distilgpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ee00a4-9250-4f67-bd51-99ed70c2dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c7368-c375-405c-b1bb-739bd7d38940",
   "metadata": {},
   "source": [
    "As before, we use the tokenizer to tokenize the text and convert each token to its token ID. We will use the `.encode` function to get the token IDs back as a Python list as they are easier to manipulate. We'll want to add extra token IDs that we've generated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27135e50-eaa7-41af-b18c-e350a9d23bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[464, 2068, 7586, 21831, 11687, 625]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(text)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29968bd0-e35b-404a-999d-744543bf6bcf",
   "metadata": {},
   "source": [
    "We can use the `tokenizer.decode` function to turn the token IDs back into text. This will be useful after we've generated further token IDs to add on the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6b272d-e7b8-4429-8a1e-05176645f311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a3386-2ac9-4829-92fb-ad711de71a90",
   "metadata": {},
   "source": [
    "Now let's run the token IDs through the `distilgpt2` model and get the probabilities of the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281c0633-374b-441d-ac16-febbef84bff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch # We'll load Pytorch so we can convert a list to a tensor\n",
    "from scipy.special import softmax\n",
    "\n",
    "as_tensor = torch.tensor(input_ids).reshape(1,-1) # This converts the token ID list to a tensor\n",
    "output = model(input_ids=as_tensor) # We pass it into the model\n",
    "next_token_scores = output.logits[0,-1,:].detach().numpy() # We get the scores for next token and the end of the sequence (token index=-1)\n",
    "next_token_probs = softmax(next_token_scores) # And we apply a softmax function\n",
    "\n",
    "next_token_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c57a869-b1dd-498b-836b-8045c3279963",
   "metadata": {},
   "source": [
    "Now we've got the probabilities for all possible 50257 tokens to be after our input text sequence.\n",
    "\n",
    "Let's get the one with the highest probability. For that we can use the `argmax` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72ea9c6-843c-45c5-891c-d9a5f14659eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_id = next_token_probs.argmax()\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67e135-94f8-4925-8a46-416d3f9429bd",
   "metadata": {},
   "source": [
    "Hmm, the token with ID=262 has the highest probability. But what token is that? `tokenizer.decode` can tell us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7395166-d938-48b6-9467-a2dad5cef9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82fbfe-115c-440d-95d5-56e57b3f1a7c",
   "metadata": {},
   "source": [
    "Now, we've all the parts we need. Your task is to calculate the next eight tokens after `input_ids` (including the one we calculated above). You'll be adding `1353` to the input token IDs, running it through the model again and deciding the next token. Try writing it as a loop that iterates eight times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a458e3b5-e341-4122-a9cf-e5a750e1321e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82042b79-aa01-4e71-88e0-7ca31f052db2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19409b88-5e3f-449d-a23e-c0b1e3a41473",
   "metadata": {},
   "source": [
    "Now picking the token with highest probability every time can often create quite boring text. Sampling from the tokens can generate more interesting text. Sampling uses the probabilities as weights so that words with higher probabilities are more likely to be chosen. Let's see how that works:\n",
    "\n",
    "Let's imagine we've got a probabilities for four possible tokens (a very tiny vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3479b7-dc91-4297-8ccf-a6a387915ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # We're using numpy to use its argmax function\n",
    "\n",
    "next_token_probs = np.array([0.1, 0.2, 0.5, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b13cf43-2ad5-4a8b-bd7f-32adf9a68ff2",
   "metadata": {},
   "source": [
    "As we saw above, we can use `argmax` that tells us the index of the highest value. In this case, it's index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e0ca01c-90d7-43c6-bc64-b5f0bc9a7095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_probs.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313efab-eb8e-4436-b623-a6791c993a26",
   "metadata": {},
   "source": [
    "However, let's say we want to sample randomly from the possible token indices (`[0, 1, 2, 3]`). First, let's create that list to sample from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6681cd40-7f95-4c86-bcac-c88868d78b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = list(range(len(next_token_probs)))\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ce123b-0ff1-42cf-81df-d0f3bb98ab28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "next_token_id = random.choices(indices, k=1)[0]\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfcca8-c397-4f2f-9738-ed126ca0b697",
   "metadata": {},
   "source": [
    "Or we could provide weights, such that some of the tokens are more likely to be chosen than others. In this case, we provide `next_token_probs` as weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e18a2a9-1b4a-48e6-b8f3-abb14c84533f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_id = random.choices(indices, k=1, weights=next_token_probs)[0]\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15ecd8-6e22-493b-bc43-4c7454a28257",
   "metadata": {},
   "source": [
    "That would allow us to sample from the token probability distribution.\n",
    "\n",
    "Your task is to generate some new text (starting from \"The quick brown fox jumped over\" as before) using sampling and the `random.choices` function to pick your next token. Try it with weighting and without weighting to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24747f-570b-4234-b5b1-19a167633cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eb1cade-fd69-447e-8976-a43b117dd7ec",
   "metadata": {},
   "source": [
    "Try running your code again and you should get a different output due to the random nature of the sampling. There's a lot of tweaks that can be made to the random sampling strategy.\n",
    "\n",
    "Fortunately, we don't have to implement all the different text generation functions ourselves. The HuggingFace library provides a `text-generation` pipeline to generate text.\n",
    "\n",
    "For example, here is how to run it and request 30 extra tokens and 5 different generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f3a47b-9a96-43cb-be4f-11ba7e7d4b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but I just wanted to see how it works.\"},\n",
       " {'generated_text': \"Hello, I'm a language model, one that is not going to be used by everyone on the web or in any other part of the web, but should be the default language. If\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'd like to thank everyone who's mentioned and helped me create it, including you, the author, everyone at the local and corporate level.\"},\n",
       " {'generated_text': \"Hello, I'm a language model,‚Äù I'd rather just say this: I hope you can learn more!Thanks~Kara\\n\\nAdvertisements\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I've been doing this for over twenty years, and I think that's really important in keeping my software stable on demand.\\n\\n\\nSo\"}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model=\"distilgpt2\")\n",
    "generator(\"Hello, I'm a language model,\", max_new_tokens=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85413bfa-d885-4670-bedc-aadffdad3234",
   "metadata": {},
   "source": [
    "There are a lot of different options, including controlling how sampling is done. If we wanted to not do sampling, we could turn it off with `do_sample=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c81a2-ce70-4078-84b3-c2ad4199ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"Hello, I'm a language model,\", max_new_tokens=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab603c4-6488-45b2-85a7-f4cd1740d4a9",
   "metadata": {},
   "source": [
    "Or turn it on but tell it to only sample from the 10 most likely tokens, we can use `do_sample=True` and `top_k=10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc50a993-2f42-44e8-9850-aa385560fbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, so I'll probably do a bit more in the future to help you understand how you're going to understand the concept of the Language.\\n\\n\\n\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Hello, I'm a language model,\", max_new_tokens=30, do_sample=True, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e67f3-974c-403b-be7d-919b0eeeab79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7473c-b162-496c-9e2d-bc90f550d8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
