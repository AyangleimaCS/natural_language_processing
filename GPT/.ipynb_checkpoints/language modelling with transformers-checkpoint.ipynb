{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b90421-f17e-4632-86b7-395815b64734",
   "metadata": {},
   "source": [
    "# Lab 2: Language Modelling with Transformers\n",
    "\n",
    "We're going to feed some text into a Transformer and examine how it outputs the probabilities for the next word/token.\n",
    "\n",
    "First let's load up the `distilgpt2` tokenizer as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93e31ef-8c27-4d3c-9740-da47f1874ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c924b5-26d7-4367-950b-c3cd18517425",
   "metadata": {},
   "source": [
    "We're going to be interested in predicting the next subword token. How many possible subword tokens are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49de9ac5-b4bb-41b2-be95-390195a360fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab) # or we could just use len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561de89-c155-4ad9-b3ec-83a7c65ba3fb",
   "metadata": {},
   "source": [
    "When tokenizing, we'll use the tokenizer with the `return_tensors='pt'` parameter. This puts the data into the format of a [PyTorch](https://pytorch.org) tensor which is used as the input for a Transformer model. PyTorch is a commonly used library for deep learning and HuggingFace builds upon it. We won't use PyTorch directly.\n",
    "\n",
    "Let's tokenize: `\"A horse! a horse! my kingdom for a\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef05bff1-712f-4901-9865-fd5d46b86f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   32,  8223,     0,   257,  8223,     0,   616, 13239,   329,   257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer('A horse! a horse! my kingdom for a', return_tensors='pt')\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99269186-3160-4e40-b1e9-ce4e9267750d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf145b-a902-4cd1-959e-1a14b27fba6b",
   "metadata": {},
   "source": [
    "Now we need to load up the full Transformer model. We need to use the same one that matches our tokenizer (`distilgpt2`). Tokenizers and models must match.\n",
    "\n",
    "We'll load it using `AutoModelForCausalLM`. CausalLM is causal language modelling, or predicting the next token. You can also load models for other purposes like document classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a06d73a-a185-439d-a6e0-51cd3c5bf5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb704eca177040399d720275c74d6e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda2024\\envs\\ml_env_v1\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745d440c82ef4cc880837241841de4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ac134-75ec-4a07-b496-364fde3df3b4",
   "metadata": {},
   "source": [
    "Now let's pass the tokenized text into the Transformer model. We could do this with `model(input_ids=tokenized['input_ids'], attention_mask=tokenized['attention_mask'])` but a tidied shorthand is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13ebc83-e5da-487b-9db3-4924af99d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4713725-6a08-4644-8201-8124789fe1e4",
   "metadata": {},
   "source": [
    "For causal language modelling, what we care about is the predictions of the next token. This is captured by the `logits` which are the scores for each of the possible tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be184fc2-791a-4143-8e26-c8d4f59ff7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-31.1439, -29.1282, -30.8418,  ..., -42.3130, -42.1440, -31.0009],\n",
       "         [-59.5865, -60.5802, -64.7680,  ..., -70.8866, -65.8933, -63.0499],\n",
       "         [-62.7691, -63.7441, -64.5699,  ..., -75.1833, -72.3488, -60.4002],\n",
       "         ...,\n",
       "         [-51.0393, -59.1055, -63.8448,  ..., -68.9364, -65.0198, -59.6002],\n",
       "         [-56.1765, -60.0481, -63.8827,  ..., -66.6802, -65.5936, -61.3876],\n",
       "         [-63.7612, -64.7149, -67.7764,  ..., -75.3739, -69.5853, -65.8060]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b4637-6c0c-490b-af7b-0ee3e097e3cd",
   "metadata": {},
   "source": [
    "This is a PyTorch tensor which is a grid of numbers. In this case, it's a 3D grid. You can see the dimensions of it using `.shape` as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0636fe65-5c13-4500-b39d-e44d46741049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 50257])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40ccbe-d6c6-4aa3-a02a-c20d81a7f9f5",
   "metadata": {},
   "source": [
    "Where do the different numbers come from?\n",
    "\n",
    "Well we only put in one sequence of ten words, so that explains the `[1, 10,...]`. The `50257` is the size of the vocabulary of the tokenizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524684d-49b5-4605-bb7c-e9a9bd136a7c",
   "metadata": {},
   "source": [
    "That means we can get the score that the Transformer has given to token `horse` after the final token in the sequence with. First, what is the token index for horse? Recall that as it is starting a new word, there is the special character of `Ġ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "360af4b5-f512-4043-a9cc-7a4c8c46b86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8223"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['Ġhorse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e0828-d31e-4340-a684-0dc41f2c67b5",
   "metadata": {},
   "source": [
    "Then to get the score from the first sequence (0), after the final token (-1) and for the token `horse` (8223), we would access it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4cafa32-9abe-48df-8a12-915a9175a728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-59.6236, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits[0,-1,8223]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1dc8e4-f4e0-47ab-8a5d-cba8d094b147",
   "metadata": {},
   "source": [
    "Hmm, the logits are not nicely probabilities so are difficult to interpret. We'll have to do a little work to make them interpretable.\n",
    "\n",
    "Let's get all the scores out for predictions of tokens after our input (so using the index of -1 to get the final logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5516d8a-2abc-4c88-a5be-31db4eb0da28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_scores = output.logits[0,-1,:].tolist()\n",
    "len(next_token_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ab48c-290f-434e-a1b0-29612f3b656d",
   "metadata": {},
   "source": [
    "They are not easy to interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51578c30-0244-48f6-b27d-95c2d7277cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-63.76122283935547,\n",
       " -64.7149429321289,\n",
       " -67.77638244628906,\n",
       " -67.36962890625,\n",
       " -67.97137451171875]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_scores[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315578ae-a50b-4311-8656-65f387a1c616",
   "metadata": {},
   "source": [
    "So we shall use a softmax function. It takes a list of numbers, applies the equation below to them (using lots of exponentials) and returns a vector where all the values are between 0 and 1 and they all add up to 1.\n",
    "\n",
    "$ softmax(z) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K $\n",
    "\n",
    "There is a [function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html) in the useful [scipy package](https://scipy.org/) that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbbf84bd-f082-43c7-9e98-ddfe85a1ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9d301-5d41-4e09-811b-208533bf769f",
   "metadata": {},
   "source": [
    "Apply the `softmax` function to `next_token_scores` and output the first five values. You should see that they are between 0 and 1 and rather small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f084a0c-b889-428e-848a-91cdd82bf718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.37544418e-05, 3.61240537e-05, 1.69133716e-06, ...,\n",
       "       8.48495937e-10, 2.77095034e-07, 1.21321184e-05])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilitites = softmax(next_token_scores)\n",
    "probabilitites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25d21e55-3cfc-41b5-b8ea-32b65979673b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proabbilities should add up to 1 or close\n",
    "probabilitites.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a861db4-d9ef-4041-a834-967fba3e023c",
   "metadata": {},
   "source": [
    "Let's see what the probability of horse is now (token id = 8223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be8ae1ff-9862-44f2-9cab-1e38d8eee2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005873771933630023"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilitites[8223]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd7eb4-02eb-4f0c-873d-202fd3b160de",
   "metadata": {},
   "source": [
    "\n",
    "You should find that it has a probability of approximately `0.006`.\n",
    "\n",
    "If we didn't already know that 8223 is horse, we could decode it with the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "260466cc-0530-44fe-a1d4-140b6be7729f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' horse'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(8223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ca092-8111-4f37-bdd4-12de963f6931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f7b0b8-939b-4c8d-995a-ab926cbdb64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa2d23-8044-4c74-bbcb-aeb261901eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
