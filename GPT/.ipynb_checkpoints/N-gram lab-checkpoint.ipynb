{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6662a78-8653-4e10-8356-5a3e4346767b",
   "metadata": {},
   "source": [
    "# Lab 1: Building own n-gram language model\n",
    "\n",
    "In this lab, build a language model based on n-grams and the Maximum Likelihood Estimation.\n",
    "\n",
    "Assume that the text is already \"tokenized\" (split up into words). We'll cover this process in more depth in the next module.\n",
    "\n",
    "As an example, let's work with two sentences from \"[The Disappearance of Lady Frances Carfax](https://en.wikipedia.org/wiki/The_Disappearance_of_Lady_Frances_Carfax)\", a short story written by [Sir Arthur Conan Doyle](https://en.wikipedia.org/wiki/Arthur_Conan_Doyle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db60837-c570-455d-bda6-15a8606794f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens for the sentence \"It shows, my dear Watson, that we are dealing\n",
    "# with an exceptionally astude and dangerous man.\"\n",
    "sample1 = ['It', 'shows', ',', 'my', 'dear', 'Watson', ',', 'that',\n",
    "           'we', 'are', 'dealing', 'with', 'an', 'exceptionally',\n",
    "           'astute', 'and', 'dangerous', 'man', '.']\n",
    "# Tokens for the sentence \"How would Lausanne do, my dear Watson?\"\n",
    "sample2 = ['How', 'would', 'Lausanne', 'do', ',', 'my', 'dear',\n",
    "           'Watson', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9f357-9fd8-417d-ae9e-f7908b446655",
   "metadata": {},
   "source": [
    "Your first task is to write a function that splits the `tokens` sequence\n",
    "into its `n`-grams.\n",
    "\n",
    "For instance, when `tokens=sample1` and `n=3`, your function should\n",
    "return:\n",
    "\n",
    "```python\n",
    "[('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.')]\n",
    "```\n",
    " \n",
    "Note: You should return a python [`list`](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists) containing [`tuple`](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences)s. `tuple`s are immutable sequences, which will be useful later on when you build your language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8a10db-dfd0-44ff-b79a-f1f0b4518c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln:  8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('How', 'would'),\n",
       " ('would', 'Lausanne'),\n",
       " ('Lausanne', 'do'),\n",
       " ('do', ','),\n",
       " (',', 'my'),\n",
       " ('my', 'dear'),\n",
       " ('dear', 'Watson'),\n",
       " ('Watson', '?')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "def build_ngrams(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    result =[]\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        tmp = tokens[i: i+n]\n",
    "        result.append(tuple(tmp))\n",
    "    return result\n",
    "# Example:\n",
    "print('ln: ', len(build_ngrams(sample2, n=2)))\n",
    "build_ngrams(sample2, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11b112-56e2-44b8-9ce2-82f02c5cf7f5",
   "metadata": {},
   "source": [
    "With the current function, there's no way to know whether an n-gram is at the beginning, middle, or end of the sequence. To overcome this problem, n-gram language models often include special \"beginning-of-string\" (BOS) and \"end-of-string\" (EOS) control tokens.\n",
    "\n",
    "Write a new version of your `build_ngrams` function that includes these control tokens. For instance, when `tokens=sample1` and `n=3`, your new function should return:\n",
    "\n",
    "```python\n",
    "[('<BOS>', '<BOS>', 'It'),\n",
    " ('<BOS>', 'It', 'shows'),\n",
    " ('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.'),\n",
    " ('man', '.', '<EOS>'),\n",
    " ('.', '<EOS>', '<EOS>')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abdffbf4-b03e-4cfa-8e5b-6aedf130dc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens :  ['<BOS>', 'It', 'shows', ',', 'my', 'dear', 'Watson', ',', 'that', 'we', 'are', 'dealing', 'with', 'an', 'exceptionally', 'astute', 'and', 'dangerous', 'man', '.', '<EOS>']\n",
      "final len:  20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<BOS>', 'It'),\n",
       " ('It', 'shows'),\n",
       " ('shows', ','),\n",
       " (',', 'my'),\n",
       " ('my', 'dear'),\n",
       " ('dear', 'Watson'),\n",
       " ('Watson', ','),\n",
       " (',', 'that'),\n",
       " ('that', 'we'),\n",
       " ('we', 'are'),\n",
       " ('are', 'dealing'),\n",
       " ('dealing', 'with'),\n",
       " ('with', 'an'),\n",
       " ('an', 'exceptionally'),\n",
       " ('exceptionally', 'astute'),\n",
       " ('astute', 'and'),\n",
       " ('and', 'dangerous'),\n",
       " ('dangerous', 'man'),\n",
       " ('man', '.'),\n",
       " ('.', '<EOS>')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "def build_ngrams_ctrl(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    tokens_copy = tokens.copy()\n",
    "\n",
    "    lenn = len(tokens_copy)\n",
    "    for i in range(n-1):\n",
    "        tokens_copy.insert(i, BOS)\n",
    "    new_end = lenn+n-1\n",
    "    for j in range(new_end, new_end+n-1):\n",
    "        tokens_copy.insert(j, EOS)\n",
    "#     tokens_copy.insert(lenn+3, EOS)\n",
    "    print('tokens : ', tokens_copy)\n",
    "    result = build_ngrams(tokens_copy, n)\n",
    "    return result\n",
    "    # Example:\n",
    "res = build_ngrams_ctrl(sample1, n=2)\n",
    "print('final len: ', len(res))\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681c182-0f08-43c8-a86b-f3d759072f68",
   "metadata": {},
   "source": [
    "Now that you can build n-grams, we have almost everything we need to build an n-gram language model.\n",
    "\n",
    "To compute Maximum Likelihood Estimations, you first need to count the number of times each word follows an n-gram of size `n-1`. You can build this structure as a Python [`dict`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) that maps the n-grams of size `n-1` to another `dict` that maps the following words to their respective counts.\n",
    "\n",
    "For instance, when `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
    "    ('<BOS>', 'It'): {'shows': 1},\n",
    "    ('<BOS>', 'How'): {'would': 1},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 2},\n",
    "    ('dear', 'Watson'): {',': 1, '?': 1},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6a37b22-1071-466e-a05f-495da9792a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens :  ['<BOS>', '<BOS>', 'It', 'shows', ',', 'my', 'dear', 'Watson', ',', 'that', 'we', 'are', 'dealing', 'with', 'an', 'exceptionally', 'astute', 'and', 'dangerous', 'man', '.', '<EOS>', '<EOS>']\n",
      "tokens :  ['<BOS>', '<BOS>', 'How', 'would', 'Lausanne', 'do', ',', 'my', 'dear', 'Watson', '?', '<EOS>', '<EOS>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
       " ('<BOS>', 'It'): {'shows': 1},\n",
       " ('It', 'shows'): {',': 1},\n",
       " ('shows', ','): {'my': 1},\n",
       " (',', 'my'): {'dear': 2},\n",
       " ('my', 'dear'): {'Watson': 2},\n",
       " ('dear', 'Watson'): {',': 1, '?': 1},\n",
       " ('Watson', ','): {'that': 1},\n",
       " (',', 'that'): {'we': 1},\n",
       " ('that', 'we'): {'are': 1},\n",
       " ('we', 'are'): {'dealing': 1},\n",
       " ('are', 'dealing'): {'with': 1},\n",
       " ('dealing', 'with'): {'an': 1},\n",
       " ('with', 'an'): {'exceptionally': 1},\n",
       " ('an', 'exceptionally'): {'astute': 1},\n",
       " ('exceptionally', 'astute'): {'and': 1},\n",
       " ('astute', 'and'): {'dangerous': 1},\n",
       " ('and', 'dangerous'): {'man': 1},\n",
       " ('dangerous', 'man'): {'.': 1},\n",
       " ('man', '.'): {'<EOS>': 1},\n",
       " ('.', '<EOS>'): {'<EOS>': 1},\n",
       " ('<BOS>', 'How'): {'would': 1},\n",
       " ('How', 'would'): {'Lausanne': 1},\n",
       " ('would', 'Lausanne'): {'do': 1},\n",
       " ('Lausanne', 'do'): {',': 1},\n",
       " ('do', ','): {'my': 1},\n",
       " ('Watson', '?'): {'<EOS>': 1},\n",
       " ('?', '<EOS>'): {'<EOS>': 1}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def count_ngrams(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, int]]:\n",
    "    result = {}\n",
    "    for sample in texts:\n",
    "        res = build_ngrams_ctrl(sample, n)\n",
    "        for subtext in res:\n",
    "            preseq = subtext[0:n-1]\n",
    "            if preseq not in result:\n",
    "                next_word_count = {}\n",
    "                next_word_count[subtext[n-1]] = 1\n",
    "                result[preseq] = next_word_count\n",
    "            else:\n",
    "                next_word_count = result[preseq]\n",
    "                if subtext[n-1] in next_word_count:\n",
    "                    next_word_count[subtext[n-1]] += 1\n",
    "                else:\n",
    "                    next_word_count[subtext[n-1]] = 1\n",
    "                result[preseq] = next_word_count\n",
    "    return result \n",
    "\n",
    "# Example:\n",
    "count_ngrams([sample1, sample2], n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ce69c-65fe-49d5-b79d-dbb002625fda",
   "metadata": {},
   "source": [
    "You're almost there! The last step is to convert the counts into probability estimates.\n",
    "\n",
    "When `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
    "    ('<BOS>', 'It'): {'shows': 1.0},\n",
    "    ('<BOS>', 'How'): {'would': 1.0},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 1.0},\n",
    "    ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7963999-a51d-45b2-9b7c-043cdd6b5bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens :  ['<BOS>', '<BOS>', 'It', 'shows', ',', 'my', 'dear', 'Watson', ',', 'that', 'we', 'are', 'dealing', 'with', 'an', 'exceptionally', 'astute', 'and', 'dangerous', 'man', '.', '<EOS>', '<EOS>']\n",
      "tokens :  ['<BOS>', '<BOS>', 'How', 'would', 'Lausanne', 'do', ',', 'my', 'dear', 'Watson', '?', '<EOS>', '<EOS>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
       " ('<BOS>', 'It'): {'shows': 1.0},\n",
       " ('It', 'shows'): {',': 1.0},\n",
       " ('shows', ','): {'my': 1.0},\n",
       " (',', 'my'): {'dear': 1.0},\n",
       " ('my', 'dear'): {'Watson': 1.0},\n",
       " ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
       " ('Watson', ','): {'that': 1.0},\n",
       " (',', 'that'): {'we': 1.0},\n",
       " ('that', 'we'): {'are': 1.0},\n",
       " ('we', 'are'): {'dealing': 1.0},\n",
       " ('are', 'dealing'): {'with': 1.0},\n",
       " ('dealing', 'with'): {'an': 1.0},\n",
       " ('with', 'an'): {'exceptionally': 1.0},\n",
       " ('an', 'exceptionally'): {'astute': 1.0},\n",
       " ('exceptionally', 'astute'): {'and': 1.0},\n",
       " ('astute', 'and'): {'dangerous': 1.0},\n",
       " ('and', 'dangerous'): {'man': 1.0},\n",
       " ('dangerous', 'man'): {'.': 1.0},\n",
       " ('man', '.'): {'<EOS>': 1.0},\n",
       " ('.', '<EOS>'): {'<EOS>': 1.0},\n",
       " ('<BOS>', 'How'): {'would': 1.0},\n",
       " ('How', 'would'): {'Lausanne': 1.0},\n",
       " ('would', 'Lausanne'): {'do': 1.0},\n",
       " ('Lausanne', 'do'): {',': 1.0},\n",
       " ('do', ','): {'my': 1.0},\n",
       " ('Watson', '?'): {'<EOS>': 1.0},\n",
       " ('?', '<EOS>'): {'<EOS>': 1.0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def build_ngram_model(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, float]]:\n",
    "    count_ngram_dict = count_ngrams(texts, n)\n",
    "    ngram_result = {}\n",
    "    for k,v in count_ngram_dict.items():\n",
    "        total_sum = sum(v.values())\n",
    "        res = {key: val/total_sum for key,val in v.items()}\n",
    "        ngram_result[k] = res\n",
    "    return ngram_result\n",
    "\n",
    "# Example:\n",
    "build_ngram_model([sample1, sample2], n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d1766-a316-4513-bff4-9f25c5e4277e",
   "metadata": {},
   "source": [
    "A language model built from only a few sentences is not very informative. Let's scale up and see what your language model looks like when we train on the complete works of Sir Arthur Conon Doyle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c137b60-8992-4837-b8b4-23dd52da72c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'arthur-conan-doyle.tok.train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m full_text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marthur-conan-doyle.tok.train.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fin:\n\u001b[0;32m      4\u001b[0m         full_text\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(line\u001b[38;5;241m.\u001b[39msplit()))\n",
      "File \u001b[1;32m~\\anaconda2024\\envs\\ml_env_v1\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'arthur-conan-doyle.tok.train.txt'"
     ]
    }
   ],
   "source": [
    "full_text = []\n",
    "with open('arthur-conan-doyle.tok.train.txt', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        full_text.append(list(line.split()))\n",
    "model = build_ngram_model(full_text, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e7be64d-8ea8-4187-b1a8-8e14008c295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <BOS>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m [(BOS, BOS), (BOS, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIt\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwas\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdear\u001b[39m\u001b[38;5;124m'\u001b[39m)]:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39mprefix)\n\u001b[1;32m----> 3\u001b[0m     sorted_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mmodel\u001b[49m[prefix]\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m-\u001b[39mx[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m sorted_probs[:\u001b[38;5;241m5\u001b[39m]:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for prefix in [(BOS, BOS), (BOS, 'It'), ('It', 'was'), ('my', 'dear')]:\n",
    "    print(*prefix)\n",
    "    sorted_probs = sorted(model[prefix].items(), key=lambda x: -x[1])\n",
    "    for k, v in sorted_probs[:5]:\n",
    "        print(f'\\t{k}\\t{v:.4f}')\n",
    "    print(f'\\t[{len(sorted_probs)-5} more...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998e542-eb18-4d85-9a74-88043f94c9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d744c1-3a1b-4efd-a56f-75333246f602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
